<img width="1604" height="1030" alt="C5308E27-52ED-4888-8300-BFAA4445CB71" src="https://github.com/user-attachments/assets/7c8f1f36-b7d6-40a5-b391-70de142e4cd0" />


ğŸ“Š CSV to BigQuery Data Pipeline (GCP + Cloud Run + Power BI)
ğŸš€ Overview

This project demonstrates an end-to-end data ingestion and analytics pipeline built on Google Cloud Platform (GCP).
The pipeline ingests CSV files sent via email, processes them using a Python application running on Cloud Run, loads the data into BigQuery, and finally enables reporting and visualization in Power BI.

This architecture is designed to be serverless, scalable, and cost-efficient, making it ideal for automated batch ingestion use cases.

ğŸ—ï¸ Architecture

Flow:

CSV Files (Email Input)

Users send CSV files (e.g. reports, transactions, logs) via email.

Files are collected and passed to the ingestion service.

Cloud Run (Python Service)

A containerized Python application runs on Cloud Run.

Responsible for:

Validating CSV files

Cleaning and transforming data

Schema alignment

Loading data to BigQuery

BigQuery (Data Warehouse)

Processed data is stored in BigQuery tables.

Optimized for analytics and BI workloads.

Power BI (Analytics & Reporting)

Power BI connects directly to BigQuery.

Dashboards and reports are built on top of curated datasets.

âš™ï¸ Tech Stack
Layer	Technology
Ingestion	CSV via Email
Processing	Python
Compute	Google Cloud Run
Data Warehouse	BigQuery
Visualization	Power BI
Cloud Platform	GCP
ğŸ§  Key Features

âœ… Serverless processing (no VM management)

âœ… Auto-scaling with Cloud Run

âœ… Secure and reliable ingestion

âœ… Schema-controlled BigQuery loading

âœ… BI-ready datasets

âœ… Cost-efficient architecture

ğŸ“‚ Project Structure (Example)
.
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py            # Cloud Run entrypoint
â”‚   â”œâ”€â”€ processor.py       # CSV processing logic
â”‚   â”œâ”€â”€ bigquery_loader.py # BQ load functions
â”‚   â””â”€â”€ utils.py
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md

ğŸ”„ Data Processing Logic

Receive CSV file

Validate columns & data types

Clean nulls and bad records

Transform data (if needed)

Load to BigQuery using google-cloud-bigquery

Return success/failure response

ğŸ§ª Example Use Cases

Daily sales reports ingestion

Finance data pipelines

Operational metrics ingestion

Automated reporting pipelines

Analytics-ready data for BI tools

ğŸš€ Deployment (Cloud Run)
gcloud builds submit --tag gcr.io/PROJECT_ID/csv-pipeline
gcloud run deploy csv-pipeline \
  --image gcr.io/PROJECT_ID/csv-pipeline \
  --platform managed \
  --region us-central1 \
  --allow-unauthenticated

ğŸ“ˆ Visualization

Power BI connects to BigQuery using:

DirectQuery (real-time dashboards)

Import mode (optimized performance)

ğŸ” Security Best Practices

Service Account with least-privilege access

BigQuery IAM roles limited to dataset level

Secrets managed via GCP Secret Manager

HTTPS-only Cloud Run endpoint

ğŸ“ Future Improvements

Add Cloud Scheduler for automation

Add Pub/Sub for event-driven ingestion

Data quality checks (Great Expectations)

DBT transformations in BigQuery

Partitioned & clustered tables

ğŸ‘¨â€ğŸ’» Author

Ian Tristan
Aspiring Data Engineer | Cloud & Analytics
GCP â€¢ AWS â€¢ Azure â€¢ Python â€¢ SQL â€¢ BigQuery â€¢ Power BI
